{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='data/shakespeare.txt'\n",
    "BATCH_SIZE=25\n",
    "LAYER_NUM=2\n",
    "SEQ_LENGTH=50\n",
    "HIDDEN_DIM=128\n",
    "GENERATE_LEN=500\n",
    "NB_EPOCH=1500\n",
    "MODE='train'\n",
    "WEIGHTS=''\n",
    "LEARNING_RATE=2e-3\n",
    "IS_TRAIN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data():\n",
    "    with open(DATA_DIR,\"r\") as text_file:\n",
    "        data=text_file.read()\n",
    "#         tr_data,va_data=data[0:1070392],data[1070392:1075392]\n",
    "    lis=list(set(data))    \n",
    "    total_char,unique_char=len(data),len(lis)\n",
    "    print ('data has %d characters, %d unique.' %(total_char, unique_char))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(lis) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(lis) }\n",
    "    return data\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "data=get_text_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the char-to-id mappings that i used earlier and stored in this file, used for training if stopped at some time\n",
    "\n",
    "with open('data/coding2.json') as file:\n",
    "    da=json.load(file)\n",
    "    total_chars,unique_chars,char_id,id_char=da['total'],da['unique'],da['char_id'],da['id_char']\n",
    "\n",
    "id_v={int(k):v for k,v in id_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_batch(given_data, seq_length, p):\n",
    "    inputs = [char_id[ch] for ch in given_data[p:p+seq_length]]\n",
    "    targets = [char_id[ch] for ch in given_data[p+1:p+seq_length+1]]\n",
    "    inputs=np.array(inputs).reshape(1,seq_length)\n",
    "    targets=np.array(targets).reshape(1,seq_length)\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing- generated all the batches at once rather than dynamic generation\n",
    "ptr=0\n",
    "batches=(total_chars//SEQ_LENGTH)\n",
    "x=[]\n",
    "y=[]\n",
    "for batch in range(batches):\n",
    "    a,b=generate_next_batch(data, SEQ_LENGTH, ptr)\n",
    "    x.append(a)\n",
    "    y.append(b)\n",
    "    ptr+=SEQ_LENGTH\n",
    "x=np.array(x).reshape(batches, SEQ_LENGTH)\n",
    "y=np.array(y).reshape(batches, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44615, 25)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smaple generator function here, tbh this is a hack where i feed a sequence of 50 chars but just \n",
    "#use the first one and replace it, if any way to reset the x and y tensors- definitely help\n",
    "def generator(seed_i,n,cu_state):\n",
    "    x= seed_i\n",
    "    ixes=[]\n",
    "    ixes.append(seed_i[0][0])\n",
    "    global IS_TRAIN\n",
    "    IS_TRAIN=False\n",
    "    for t in range(n):\n",
    "        p,cu_state=sess.run([prediction,states],{X:x,_initial_state:cu_state})\n",
    "        ix = np.random.choice(range(unique_chars), p=p[0].ravel())\n",
    "        x[0][0]=ix\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM(x):\n",
    "    with tf.name_scope('lstm'):\n",
    "        cells= [tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN_DIM) for layer in range(LAYER_NUM)]\n",
    "        multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "        return multi_layer_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# building the graph, again i wanted this to be as simple as andrej's code so didnt want to implement this as a class,\n",
    "# but i have uploaded a different class based version in the second ipython file\n",
    "reset_graph()\n",
    "\n",
    "X=tf.placeholder(tf.int32,[BATCH_SIZE,SEQ_LENGTH],name='inputs')\n",
    "Y=tf.placeholder(tf.int32,[BATCH_SIZE,SEQ_LENGTH],name='targets')\n",
    "\n",
    "X_onehot=tf.one_hot(X,unique_chars)\n",
    "Y_onehot=tf.one_hot(Y,unique_chars)\n",
    "\n",
    "multi_layer_cell=build_LSTM(X_onehot)\n",
    "\n",
    "_initial_state =  multi_layer_cell.zero_state(BATCH_SIZE, tf.float32)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X_onehot,initial_state=_initial_state,dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('mine'):\n",
    "    W=tf.Variable(tf.random_normal((HIDDEN_DIM,unique_chars),stddev=0.1))\n",
    "    B=tf.Variable(tf.zeros(shape=[unique_chars]))\n",
    "\n",
    "with tf.name_scope('logits'):\n",
    "    flat_outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, HIDDEN_DIM])\n",
    "    flat_y = tf.reshape(tf.concat(axis=1, values=Y_onehot), [-1,unique_chars])\n",
    "    Ys=tf.matmul(flat_outputs,W)+B\n",
    "\n",
    "with tf.name_scope('cross-entropy'):\n",
    "    prediction=tf.nn.softmax(Ys)\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=flat_y, logits=Ys)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 5)\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    minimize = optimizer.apply_gradients(zip(grads,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "no_of_batches=int(total_chars//BATCH_SIZE)\n",
    "ptr=0\n",
    "n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this was running as expected but it has some errors, and even when the loss reached 3, satisfactory results were not obtained\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     smooth_loss_tr = -np.log(1.0/unique_chars)*SEQ_LENGTH # loss at iteration 0\n",
    "#     sess.run(init)\n",
    "#     zero_state=sess.run(_initial_state)\n",
    "#     for i in range(NB_EPOCH):\n",
    "#         curr_state=zero_state\n",
    "#         for j in range(batches//BATCH_SIZE):\n",
    "#             if (ptr+BATCH_SIZE+1 >= batches): \n",
    "#                 ptr=0\n",
    "#             inputs,targets=x[ptr:ptr+BATCH_SIZE],y[ptr:ptr+BATCH_SIZE]\n",
    "#             ptr+=BATCH_SIZE\n",
    "            \n",
    "#             if(j%100==0): \n",
    "#                 sample_ix=generator(np.array(inputs),200,zero_state)\n",
    "#                 txt = ''.join(id_v[ix] for ix in sample_ix)\n",
    "#                 print ('----\\n %s \\n----' % (txt, ))\n",
    "            \n",
    "#             _,loss_t,curr_state=sess.run([minimize,loss,states],feed_dict={X:inputs,Y:targets,_initial_state:curr_state})\n",
    "#             smooth_loss_tr = smooth_loss_tr * 0.999 + loss_t * 0.001\n",
    "            \n",
    "#             if(j%100==0):\n",
    "#                 print('epoch %i iteration %i loss %f va'%(i,j,smooth_loss_tr))\n",
    "        \n",
    "#         if(i%10==0):\n",
    "#             save_path = saver.save(sess, \"finall/LSTMmodel\"+str(i)+\"loss\"+str(int(smooth_loss_tr))+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#here is where I found some trouble, as can be seen from the above cell i changed my implementation to fit it more like\n",
    "#karpathy's min char rnn but the smooth_loss drops too rapidly\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    smooth_loss= -np.log(1.0/unique_chars)*SEQ_LENGTH # loss at iteration 0\n",
    "    sess.run(init)\n",
    "    zero_state=sess.run(_initial_state)\n",
    "    curr_state=zero_state\n",
    "    while True:\n",
    "        if(ptr+BATCH_SIZE+1>=y.shape[0]):\n",
    "            curr_state=zero_state\n",
    "            ptr=0\n",
    "        inputs,targets=x[ptr:ptr+BATCH_SIZE],y[ptr:ptr+BATCH_SIZE]\n",
    "        \n",
    "        if n % 100 == 0:\n",
    "            sample_ix = generator(inputs, 200, curr_state)\n",
    "            txt = ''.join(id_v[ix] for ix in sample_ix)\n",
    "            print ('----\\n %s \\n----' % (txt, ))\n",
    "        \n",
    "        loss_t,curr_state,_=sess.run([loss, states, minimize], feed_dict={X:inputs, Y:targets,\n",
    "                                                                        _initial_state:curr_state})\n",
    "        smooth_loss = smooth_loss * 0.999 + loss_t * 0.001\n",
    "\n",
    "        if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        ptr+=BATCH_SIZE\n",
    "        n+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
