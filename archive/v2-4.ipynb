{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='data/shakespeare.txt'\n",
    "BATCH_SIZE=1\n",
    "LAYER_NUM=1\n",
    "SEQ_LENGTH=25\n",
    "HIDDEN_DIM=128\n",
    "GENERATE_LEN=500\n",
    "NB_EPOCH=1500\n",
    "MODE='train'\n",
    "WEIGHTS=''\n",
    "LEARNING_RATE=1e-1\n",
    "IS_TRAIN=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_data():\n",
    "    with open(DATA_DIR,\"r\") as text_file:\n",
    "        data=text_file.read()\n",
    "#         tr_data,va_data=data[0:1070392],data[1070392:1075392]\n",
    "    lis=list(set(data))    \n",
    "    total_char,unique_char=len(data),len(lis)\n",
    "    print ('data has %d characters, %d unique.' %(total_char, unique_char))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(lis) }\n",
    "    ix_to_char = { i:ch for i,ch in enumerate(lis) }\n",
    "    return data\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115393 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "data=get_text_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/coding2.json') as file:\n",
    "    da=json.load(file)\n",
    "    total_chars,unique_chars,char_id,id_char=da['total'],da['unique'],da['char_id'],da['id_char']\n",
    "\n",
    "id_v={int(k):v for k,v in id_char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_batch(given_data, seq_length, p):\n",
    "    inputs = [char_id[ch] for ch in given_data[p:p+seq_length]]\n",
    "    targets = [char_id[ch] for ch in given_data[p+1:p+seq_length+1]]\n",
    "    inputs=np.array(inputs).reshape(1,seq_length)\n",
    "    targets=np.array(targets).reshape(1,seq_length)\n",
    "    return inputs,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr=0\n",
    "batches=(total_chars//SEQ_LENGTH)\n",
    "x=[]\n",
    "y=[]\n",
    "for batch in range(batches):\n",
    "    a,b=generate_next_batch(data, SEQ_LENGTH, ptr)\n",
    "    x.append(a)\n",
    "    y.append(b)\n",
    "    ptr+=SEQ_LENGTH\n",
    "x=np.array(x).reshape(batches, SEQ_LENGTH)\n",
    "y=np.array(y).reshape(batches, SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44615, 25)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test generator function here\n",
    "def generator(seed_i,n,cu_state):\n",
    "    x= seed_i\n",
    "    ixes=[]\n",
    "    ixes.append(seed_i[0][0])\n",
    "    global IS_TRAIN\n",
    "    IS_TRAIN=False\n",
    "    for t in range(n):\n",
    "        p,cu_state=sess.run([prediction,states],{X:x,_initial_state:cu_state})\n",
    "        ix = np.random.choice(range(unique_chars), p=p[0].ravel())\n",
    "        x[0][0]=ix\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_LSTM(x):\n",
    "    with tf.name_scope('lstm'):\n",
    "        cells= [tf.contrib.rnn.BasicLSTMCell(num_units=HIDDEN_DIM) for layer in range(LAYER_NUM)]\n",
    "        multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "#         batch_size_T  = np.shape(x)[0]\n",
    "        return multi_layer_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X=tf.placeholder(tf.int32,[BATCH_SIZE,SEQ_LENGTH],name='inputs')\n",
    "Y=tf.placeholder(tf.int32,[BATCH_SIZE,SEQ_LENGTH],name='targets')\n",
    "\n",
    "X_onehot=tf.one_hot(X,unique_chars)\n",
    "Y_onehot=tf.one_hot(Y,unique_chars)\n",
    "\n",
    "multi_layer_cell=build_LSTM(X_onehot)\n",
    "\n",
    "_initial_state =  multi_layer_cell.zero_state(BATCH_SIZE, tf.float32)\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X_onehot,initial_state=_initial_state,dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope('mine'):\n",
    "    W=tf.Variable(tf.random_normal((HIDDEN_DIM,unique_chars),stddev=0.1))\n",
    "    B=tf.Variable(tf.zeros(shape=[unique_chars]))\n",
    "\n",
    "with tf.name_scope('logits'):\n",
    "    flat_outputs = tf.reshape(tf.concat(axis=1, values=outputs), [-1, HIDDEN_DIM])\n",
    "    flat_y = tf.reshape(tf.concat(axis=1, values=Y_onehot), [-1,unique_chars])\n",
    "    Ys=tf.matmul(flat_outputs,W)+B\n",
    "\n",
    "with tf.name_scope('cross-entropy'):\n",
    "    prediction=tf.nn.softmax(Ys)\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=flat_y, logits=Ys)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "with tf.name_scope('adam_optimizer'):\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), 5)\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "    minimize = optimizer.apply_gradients(zip(grads,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "no_of_batches=int(total_chars//BATCH_SIZE)\n",
    "ptr=0\n",
    "n=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    smooth_loss= -np.log(1.0/unique_chars)*SEQ_LENGTH # loss at iteration 0\n",
    "    sess.run(init)\n",
    "    zero_state=sess.run(_initial_state)\n",
    "    curr_state=zero_state\n",
    "    while True:\n",
    "        if(ptr+BATCH_SIZE+1>=y.shape[0]):\n",
    "            curr_state=zero_state\n",
    "            ptr=0\n",
    "        inputs,targets=x[ptr:ptr+BATCH_SIZE],y[ptr:ptr+BATCH_SIZE]\n",
    "        \n",
    "        if n % 100 == 0:\n",
    "            sample_ix = generator(inputs, 200, curr_state)\n",
    "            txt = ''.join(id_v[ix] for ix in sample_ix)\n",
    "            print ('----\\n %s \\n----' % (txt, ))\n",
    "        \n",
    "        loss_t,curr_state,_=sess.run([loss, states, minimize], feed_dict={X:inputs, Y:targets,\n",
    "                                                                        _initial_state:curr_state})\n",
    "        smooth_loss = smooth_loss * 0.999 + loss_t * 0.001\n",
    "\n",
    "        if n % 100 == 0: print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        ptr+=BATCH_SIZE\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " F?KZonnxUodWChtq.YN;ToaYi-DtyXxoqGKuFYmr;wR'MgOsy!.CjKJXIf.'L$BpiT!aFGHj.:pdZjGiwUeLGsLZzP wdvVjRNAWw'zsyMgcv3fTnIFzUF.J!FJswSApU:yA3w\n",
      "KLyQiCugRYT'oRxTMSa,HSOvSZ?zzNfhL.'XrWy3kqQ$I&\n",
      "wIPgOpVNSmJJz\n",
      "\n",
      "ZJPj \n",
      "----\n",
      "epoch 0 iteration 0 loss 104.259500 va\n",
      "----\n",
      "  y.\n",
      "I f.\n",
      "Wice argered y.\n",
      "Tou\n",
      "I vecenetf alllefryblleseg checgels he, gededes ve, h; mewcet mmest b onmes, urirs mfve, atice the\n",
      "Vit b'tankes.\n",
      "\n",
      "Sge ows, henar' ld ot lell,\n",
      "amarasemetse hed\n",
      "Seduced llere \n",
      "----\n",
      "epoch 0 iteration 100 loss 94.585942 va\n",
      "----\n",
      " hal p; t: willy ss\n",
      "NINIf m, h?\n",
      "te way. zinl tedloine Yet f-fany y, thsk'bal, m, i e oreandy otisictlly te aled o heawidl\n",
      "Aiule giapeuucke, s. on!--ay ble ch w'd e\n",
      "n:\n",
      "A?\n",
      "I ik oul\n",
      "I n ucy n:\n",
      "anain\n",
      "I ontw \n",
      "----\n",
      "epoch 0 iteration 200 loss 85.778045 va\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-1b431bde57fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'----\\n %s \\n----'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_initial_state\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcurr_state\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0msmooth_loss_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss_tr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.999\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mloss_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     smooth_loss_tr = -np.log(1.0/unique_chars)*SEQ_LENGTH # loss at iteration 0\n",
    "#     sess.run(init)\n",
    "#     zero_state=sess.run(_initial_state)\n",
    "#     for i in range(NB_EPOCH):\n",
    "#         curr_state=zero_state\n",
    "#         for j in range(batches//BATCH_SIZE):\n",
    "#             if (ptr+BATCH_SIZE+1 >= batches): \n",
    "#                 ptr=0\n",
    "#             inputs,targets=x[ptr:ptr+BATCH_SIZE],y[ptr:ptr+BATCH_SIZE]\n",
    "#             ptr+=BATCH_SIZE\n",
    "            \n",
    "#             if(j%100==0): \n",
    "#                 sample_ix=generator(np.array(inputs),200,zero_state)\n",
    "#                 txt = ''.join(id_v[ix] for ix in sample_ix)\n",
    "#                 print ('----\\n %s \\n----' % (txt, ))\n",
    "            \n",
    "#             _,loss_t,curr_state=sess.run([minimize,loss,states],feed_dict={X:inputs,Y:targets,_initial_state:curr_state})\n",
    "#             smooth_loss_tr = smooth_loss_tr * 0.999 + loss_t * 0.001\n",
    "            \n",
    "#             if(j%100==0):\n",
    "#                 print('epoch %i iteration %i loss %f va'%(i,j,smooth_loss_tr))\n",
    "        \n",
    "#         if(i%10==0):\n",
    "#             save_path = saver.save(sess, \"finall/LSTMmodel\"+str(i)+\"loss\"+str(int(smooth_loss_tr))+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.51105591621423"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smooth_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
